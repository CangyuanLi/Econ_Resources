\documentclass[14pt]{extreport}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\eq}[0]{\llap{\(\Leftrightarrow\)\qquad}}
\newcommand{\answer}[0]{\medskip \textbf{Answer:} \medskip \\}
\newcommand{\union}[0]{\cup}
\newcommand{\intersect}[0]{\cap}
\newcommand{\sumn}[0]{\sum\limits_{i=1}^n}
\newcommand{\limn}[0]{\lim_{n \to \infty}}
\newcommand{\limt}[0]{\lim_{t \to \infty}}

\title{Solutions to Chapter 3 Exercises}
\author{Cangyuan Li}
\date{\today}

\begin{document}

\maketitle

\begin{enumerate}

    \item [\textbf{3.05}] Let \( \mathbf{\widehat{e}} \) be the OLS residual from a regression of \( \mathbf{Y} \) on \( \mathbf{X} \). Find the OLS coefficient from a regression of \( \mathbf{\widehat{e}} \) on \( \mathbf{X} \). 
    
    \answer
    Step 1: Here, \( \widehat{e} \) takes the place of \( Y \) in the formula for \( \widehat{\beta} \). 

    \begin{align*}
        \widetilde{\beta} &= (X'X)^{-1} X'\widehat{e} \\
        &= (X'X)^{-1} \left[ X'Y - X'X\widehat{\beta} \right] \\
        &= (X'X)^{-1} \left[ X'Y - X'X (X'X)^{-1} X'Y \right] \\
        &= (X'X)^{-1} \left[ X'Y - X'Y \right] \\
        &= 0
    \end{align*}
    

    \item [\textbf{3.06}] Let \( \mathbf{\widehat{Y} = X(X'X)^{-1} X'Y} \). Find the OLS coefficient from a regression of \( \mathbf{\widehat{Y}} \) on \( \mathbf{X} \). 
    
    \answer
    Step 1: Plug into regression formula and expand.

    \begin{align*}
        \widetilde{\beta} &= (X'X)^{-1} X'\widehat{Y} \\
        &= \left[ (X'X)^{-1} X'X \right] (X'X)^{-1} X'Y \\
        &= I (X'X)^{-1} X'Y \\
        &= \widehat{\beta}
    \end{align*}


    \item [\textbf{3.11}] Show that when \( \mathbf{X} \) contains a constant, \( n^{-1} \sumn \widehat{Y}_i = \overline{Y} \).

    \answer
    Step 1: Note that \( Y_i = \widehat{Y}_i + \widehat{e}_i \). 
    Also, \( \overline{Y} := n^{-1} \sumn Y_i \).

    \begin{align*}
        &\sumn Y_i = \sumn \widehat{Y}_i + \sumn \widehat{e}_i \\
        &\eq n^{-1} \sumn Y_i = n^{-1} \sumn \widehat{Y}_i + n^{-1} \sumn \widehat{e}_i \\
        &\eq \overline{Y} = n^{-1} \sumn \widehat{Y}_i + n^{-1} \sumn \widehat{e}_i
    \end{align*}

    Step 2: The equation specified in the problem is then only true if \( \sumn \widehat{e}_i = 0 \). \( X \) is a matrix of the form:

    \begin{align*}
        \begin{bmatrix}
            x_{11} & x_{12} & \cdots x_{1, k-1} & 1 \\
            x_{21} & x_{22} & \cdots x_{2, k-1} & 1 \\
            \vdots & \vdots & \vdots & \vdots \\
            x_{n1} & x_{n2} & \cdots x_{n, k-1} & 1
        \end{bmatrix}
    \end{align*}

    Partition \( X \) into \( \begin{bmatrix} X^{\ast} & O \end{bmatrix}\) such that \( X^{\ast} \) is a \( n \times k-1 \) matrix of the \( x \) observations and \( O \) is a \( n \times 1 \) vector of ones. We want the sum of the residuals to be zero, and recall that we showed (in matrix form) in Exercise 3.05 that \( X'\widehat{e} = 0 \). Also note that the transpose of a partitioned matrix is like the transpose of a regular matrix except the elements are also transposed. So,

    \begin{align*}
        X'\widehat{e} &= \begin{bmatrix} 
            X^{\ast'} \\
            O'
        \end{bmatrix} \widehat{e} \\
        &= \begin{bmatrix}
            X^{\ast'} \widehat{e} \\
            O' \widehat{e} 
        \end{bmatrix} \\
        &= 0
    \end{align*}

    Then \( O'\widehat{e} = \sumn \widehat{e}_i = 0 \). 


    \item [\textbf{3.14}]
    Let \(\widehat{\beta}_n = \mathbf{(X_n'X_n)^{-1}X_n'Y_n}\) denote the OLS estimate where 
    \(\mathbf{Y_n}\) is \(n \times 1\) and \(\mathbf{X_n}\) is \(n \times k\). Prove that the OLS estimate 
    computed using an additional observation \((Y_{n+1}, X_{n+1})\) is 

    \begin{align*}
        \widehat{\beta}_{n+1} &= \widehat{\beta}_n + \frac{1}{1 + X'_{n+1}(X'_n X_n)^{-1}X_{n+1}}
        (X'_{n} X_{n})^{-1}X_{n+1}(Y_{n+1} - X'_{n+1}\widehat{\beta}_n)
    \end{align*}

    \answer
    Step 1: Apply the Woodbury Matrix Identity. Let 

    \begin{align*}
        A &= X_n'X_n \\
        B &= X_{n+1} \\
        D &= X'_{n+1}
    \end{align*}

    The choice of \(D\) is a bit arbitrary. I think setting the last term equal to \(C\) would work,
    but this way the `1' is already present and the equation is a bit easier to manipulate.
    Then, since \(C = I = 1\) because a single observation is 1 by 1.

    \begin{align*}
        (A + BCD)^{-1} &= A^{-1} - A^{-1}BC\left(C + CDA^{-1}BC\right)^{-1}CDA^{-1} \\
        &= A^{-1} - A^{-1}B(1 + DA^{-1}B)DA^{-1} \\
    \end{align*}

    Step 2: Simplify. The terms of the expanded equation are:

    \begin{align*}
        Term1 &= A^{-1}X'_nY_n \\
        &= (X'_n X_n)^{-1}X'_nY_n \\
        &= \widehat{\beta}_n
    \end{align*}

    \begin{align*}
        Term2 &= A^{-1}X_{n+1}Y_{n+1} \\
        &= (X'_n X_n)^{-1}(X_{n+1} Y_{n+1})
    \end{align*}

    \begin{align*}
        Term3 &= (X'_n X_n)^{-1} X_{n+1} \left[1 + X'_{n+1}(X'_n X_n)^{-1}X_{n+1}\right]^{-1}
                 X'_{n+1}(X'_nX_n)^{-1}X'_nY_n \\
              &= (X'_n X_n)^{-1}X_{n+1}\left[1 + X'_{n+1}(X'_n X_n)^{-1}X_{n+1}\right]^{-1}
                 X'_{n_1} \widehat{\beta}_n
    \end{align*}

    \begin{align*}
        Term4 &= (X'_n X_n)^{-1} X_{n+1} \left[1 + X'_{n+1}(X'_n X_n)^{-1}X_{n+1}\right]
                X'_{n+1}(X'_n X_n)^{-1} X_{n+1} Y_{n+1}
    \end{align*}

    Step 3: Combine terms. Note that the term starting with `1 + ...' is a scalar, call it \(L\). 
    Call the term \(Z := (X'_n X_n)^{-1} X_{n+1}\). Note also that \(L = 1 + X'_{n+1}Z\). The observations
    are also scalars and may be rearranged. So:

    \begin{align*}
        \widehat{\beta}_{n+1} &= Term1 + Term2 - Term3 - Term4 \\
        &= \widehat{\beta}_n + Z Y_{n+1} - \frac{1}{L} Z X'_{n+1}\widehat{\beta}_n - 
        \frac{1}{L} Z X'_{n+1} Z Y_{n+1} \\
        &= \widehat{\beta}_n + ZY_{n+1}\frac{L}{L} - \frac{1}{L} ZZ X'_{n+1} Y_{n+1} - 
        - \frac{1}{L} Z X'_{n+1}\widehat{\beta}_n \\
        &= \widehat{\beta}_n + \frac{1}{L}(ZY_{n+1} + ZZ X'_{n+1} Y_{n+1} - ZZ X'_{n+1} Y_{n+1}
        - Z X'_{n+1}\widehat{\beta}_n) \\
        &= \widehat{\beta}_n + \frac{1}{L}Z(Y_{n+1} - X'_{n+1}\widehat{\beta}_n)
    \end{align*}


    \item [\textbf{3.19}] For the intercept-only model \( Y_i = \beta + e_i \), show that the leave-one-out prediction error is 
    
    \begin{align*}
        \widetilde{e} = \left( \frac{n}{n-1} \right) \left( Y_i - \overline{Y} \right) 
    \end{align*}
    
\end{enumerate}

\end{document}